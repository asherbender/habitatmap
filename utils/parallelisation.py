"""Parallelisation tools.

.. sectionauthor:: Asher Bender <a.bender@acfr.usyd.edu.au>
.. codeauthor:: Asher Bender <a.bender@acfr.usyd.edu.au>Email

"""
import sys
import time
import types
import Queue
import multiprocessing


def piped_stdout_env(stdout_queue, output_queue, target, *args, **kwargs):
    """Execute a target function with stdout redirected to a queue.

    :py:func:piped_stdout_env is designed to be launched as new process. It is
    a wrapper which allows all stdout activity generated by the target function
    to be captured and funnelled into a queue.

    Args:
        stdout_queue (multiprocessing.Queue): queue used to capture stdout.
        output_queue (multiprocessing.Queue): queue used to return target
            function output.
        target (function): function to execute.
        *args: arguments to target function.
        **kwargs: optional key-word arguments to target function.

    """

    def write(self, msg):
        self.put(msg)

    def flush(self):
        sys.__stdout__.flush()

    # Monkey patch queue to behave like stdout.
    stdout_queue.write = types.MethodType(write, stdout_queue)
    stdout_queue.flush = types.MethodType(flush, stdout_queue)
    sys.stdout = stdout_queue

    # Execute function with redirected stdout.
    output = target(*args, **kwargs)

    # Send output back to the parent thread.
    output_queue.put(output)


def pool(jobs, num_processes=None, verbose=True):
    """Create a pool of processes to complete a list of jobs.

    :py:func:pool implements a pool of processes to complete a list of
    jobs. Pool is a slimmed down (less functional) version of
    :py:class:multiprocessing.Pool.

    :py:func:pool was primarily designed for use in iPython Notebooks. It
    appears that iPython Notebooks will only print stdout output in 'real-time'
    if it was generated on the main/parent processes. Output sent to stdout in
    child processes are only rendered once the child process has joined. This
    can be problematic if jobs take a long time to complete but provide
    frequent updates on stdout. :py:func:pool solves this problem by
    intercepting the output and printing them on its own (main/parent) process.

    The following is list of differences between :py:func:pool differs and
    :py:class:multiprocessing.Pool

        - :py:class:multiprocessing.Pool supports asynchronous
          operation. Although :py:func:pool executes jobs in parallel
          processes, the function blocks until all jobs have been completed.

        - :py:class:multiprocessing.Pool does not centralise the stdout output
          of its children. :py:func:pool intercepts the stdout output generated
          by its children and prints them on its own (main/parent) process.

    Example::

        # Define function to distribute.
        def f(job, rand=0):
            pid = os.getpid()
            r = np.random.uniform(low=0, high=rand)
            t = job + r
            msg = 'Job {0:0>2n}, PID {1} sleeping for {2:1.2f} seconds'
            print msg.format(job, pid, t)
            time.sleep(t)
            print 'Closing {0}'.format(pid)
            return t

        # Create a list of jobs to execute.
        jobs = list()
        for i, j in enumerate([1, 6, 4, 7, 3, 0, 9, 2, 5, 8]):
            jobs.append({'target': f, 'args': (i, ), 'kwargs': {'rand': j}})

        # Execute jobs in parallel.
        output = pool(jobs, num_processes=2)

        # Print output returned from jobs.
        print ''
        for i, t in enumerate(output):
            print 'Job {0:0>2n}, slept for {1:1.2f} seconds'.format(i, t)

    Args:
        jobs (list): list of dictionaries containing a job specification. The
            dictionary must contain a 'target' key specifying which function to
            execute on a new process. The dictionary may also contain a 'args'
            key specifying the list of input arguments to the target
            function. Similarly the dictionary may contain a 'kwargs' key
            specifying the dictionary of optional key-value pairs.
        num_processes (int, optional): maximum number of processes to run
            simultaneously The integer must be greater than one. If set to
            `None` the number of processes will be set to the number of cores
            in the system.
        verbose (bool, optional): If set to True pool activity will be printed
            to stdout.

    Raises:
        Exception: If the number of processes has not been specified and the
            number of cores on the system cannot be determined. An Exception()
            will also be thrown if the number of cores specified is less than
            or equal to one. If the list of input jobs is improperly specified
            an Exception() will be thrown.

    """

    # Return the number of CPUs in the system. May raise NotImplementedError.
    if num_processes is None:
        try:
            num_processes = multiprocessing.cpu_count()
        except NotImplementedError:
            msg = 'Could not detect the number of cores in the system.'
            raise Exception(msg)

    # Validate specified number of cores.
    else:
        # There is no point 'parallelising' a job across one or less processes.
        num_processes = int(num_processes)
        if num_processes <= 1:
            msg = 'The number of processes specified {0} must be '
            msg += 'greater than one.'
            raise Exception(msg.format(num_processes))

    # Validate jobs (attempt to 'break' things before asynchronous objects have
    # been spawned).
    if not isinstance(jobs, (list, tuple)) or len(jobs) <= 1:
        msg = 'The input jobs must be contained in a list of dictionaries. '
        msg += 'The list must contain more than one entry.'
        raise Exception(msg)

    # Ensure values in job description are valid.
    for i, job in enumerate(jobs):

        # Validate job container.
        if not type(job) is dict:
            msg = 'Job {0} is not specified as a dictionary.'
            raise Exception(msg.format(i))

        # Validate target function.
        if 'target' not in job:
            msg = 'Job {0} does not contain a target function.'
            raise Exception(msg.format(i))
        else:
            if not hasattr(job['target'], '__call__'):
                msg = 'The target function in job {0} does appear to '
                msg += 'be callable.'
                raise Exception(msg.format(i))

        # Validate input arguments to target function.
        if ('args' in job) and not isinstance(job['args'], (list, tuple)):
            msg = 'The arguments to job {0} must be specified as a list.'
            raise Exception(msg.format(i))

        # Validate input arguments to target function.
        if ('kwargs' in job) and not isinstance(job['kwargs'], (list, tuple)):
            msg = 'The optional arguments to job {0} must be '
            msg += 'specified as a list.'
            raise Exception(msg.format(i))

    # Define level of output.
    verbose = True

    # Initialise variables for managing pool.
    counter = 0
    number_of_jobs = len(jobs)
    job_ID = num_processes * [None, ]
    job_pid = num_processes * [None, ]
    processes = num_processes * [None, ]
    stdout_queues = num_processes * [None, ]
    output_queues = num_processes * [None, ]

    # Allocate list for capturing output.
    output = number_of_jobs * [None, ]

    if verbose:
        print 'Staring pool:'
        print '    {0} jobs queued in pool.'.format(number_of_jobs)
        print '    {0} processes available to pool.\n'.format(num_processes)

    # Wait for all processes to join.
    while True:
        try:
            # Iterate through processes, checking for activity.
            for i in range(num_processes):

                # Process is empty and NO jobs remain, skip this process.
                if (processes[i] is None) and (counter >= number_of_jobs):
                    continue

                # Process is empty and jobs remain, spawn new process.
                elif (processes[i] is None) and (counter < number_of_jobs):
                    job_ID[i] = counter
                    stdout_queues[i] = multiprocessing.Queue()
                    output_queues[i] = multiprocessing.Queue()

                    # Create argument list. Note that the target function may
                    # not require arguments but the wrapping function does
                    # require inputs.
                    job = jobs[counter]
                    if 'args' not in job:
                        job['args'] = list()

                    # Execute job through sand-boxed environment where stdout
                    # is caught by a queue.
                    job['args'] = list(job['args'])
                    job['args'].insert(0, job['target'])
                    job['args'].insert(0, output_queues[i])
                    job['args'].insert(0, stdout_queues[i])
                    job['target'] = piped_stdout_env

                    # Create process for job.
                    processes[i] = multiprocessing.Process(**job)
                    processes[i].daemon = True
                    processes[i].start()
                    job_pid[i] = processes[i].pid
                    counter += 1

                    # Show job has started.
                    if verbose:
                        msg = 'PID {0:n}: job {1:n} of {2:n} started.'
                        print msg.format(job_pid[i], job_ID[i] + 1,
                                         number_of_jobs)

                # Process exists, check for messages.
                else:
                    # Remove and return an item from the queue. If no item is
                    # available raise the Queue.Empty exception.
                    try:
                        while True:
                            stdout_data = stdout_queues[i].get_nowait()
                            sys.stdout.write(stdout_data)

                    except Queue.Empty:
                        pass

                    # Attempt to retrieve output from function. Note that
                    # resources will not be freed until the output has been
                    # received. If a child never returns output, the resources
                    # will never be freed and this will poll forever. A
                    # function which does not return any value still returns
                    # `None`. If the program hangs here it is more likely a
                    # child process has not closed down correctly.
                    if output_queues[i] is not None:
                        try:
                            output[job_ID[i]] = output_queues[i].get_nowait()
                            output_queues[i] = None

                            if verbose:
                                msg = 'PID {0:n}: job {1:n} output retrieved.'
                                print msg.format(job_pid[i], job_ID[i] + 1)

                        except Queue.Empty:
                            pass

                    # Free pool resources if process is not alive.
                    if (not processes[i].is_alive()) and stdout_queues[i].empty():
                        if verbose:
                            msg = 'PID {0:n}: job {1:n} finished.'
                            print msg.format(job_pid[i], job_ID[i] + 1)

                        job_ID[i] = None
                        job_pid[i] = None
                        processes[i] = None
                        stdout_queues[i] = None

        # Interrupt received, attempt to shutdown workers.
        except KeyboardInterrupt:
            print 'Pool received keyboard interrupt'
            for i, process in enumerate(processes):
                if process is not None:
                    process.terminate()
                    process.join()
                    stdout_queues[i] = None
                    output_queues[i] = None

        # Exit loop when all processes have finished execution.
        if not any(processes) and (counter >= number_of_jobs):
            break

        # Check processes for messages at 4Hz.
        time.sleep(0.25)

    if verbose:
        msg = '\nJobs completed. Pool closed.'
        print msg

    # Return the ordered output.
    return output
